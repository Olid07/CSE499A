{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olid07/CSE499A/blob/main/Paper_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSMqe5HGR8xU",
        "outputId": "195b852d-0850-43f8-97c2-cc7faa8378a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.18.0 multiprocess-0.70.15\n"
          ]
        }
      ],
      "source": [
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets"
      ],
      "metadata": {
        "id": "LgPRxrOMn-L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save into CSV"
      ],
      "metadata": {
        "id": "GUZD1tIo7eaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"imdb_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "PIxMacp5JiNf",
        "outputId": "8fbb51ec-69a6-4c68-a244-bf0af690ab73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-44a40643e513>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert to Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save as CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imdb_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(dataset['validation'])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"sci_valid_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "mtg27LcIhANo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(dataset['test'])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"sci_test_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "GdeQ28ooha5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8TQxEnEUfyA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpYRq1-v92Ly"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/Dataset/sci_papers.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VeJJPGjOKRS"
      },
      "outputs": [],
      "source": [
        "! sudo cp -v -r \"/content/drive/MyDrive/dataset/sci_papers.zip\" \"/content/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikOHNs0pSFSU"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('scientific_papers', 'pubmed')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset dowload from huggingface"
      ],
      "metadata": {
        "id": "2lnfXQLY73WS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DayQ1dwX6So"
      },
      "outputs": [],
      "source": [
        "from datasets import get_dataset_split_names\n",
        "\n",
        "get_dataset_split_names(\"scientific_papers\", 'pubmed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD62HL_lX6OP"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset['train'] ['abstract']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to drive"
      ],
      "metadata": {
        "id": "Hx1QHd2r7yhN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDaAyfnpNnXE"
      },
      "outputs": [],
      "source": [
        "%cd ..\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui9m8iub3tNp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start"
      ],
      "metadata": {
        "id": "yndD_HcO7lRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Jt3azBb3ErT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "l4qVdyEPnQXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
      ],
      "metadata": {
        "id": "-VMv7t_hUHFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNSLmTXvFeON"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import  Path\n",
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline , set_seed, AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlrxRsB_C4LW"
      },
      "outputs": [],
      "source": [
        "! sudo cp -v -r \"/content/drive/MyDrive/Dataset/sci_papers.zip\" \"/content/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aizoh007Dnj8"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/dataset/sci_papers.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/dataset/sci_papers/sci_valid_dataset.csv\")\n"
      ],
      "metadata": {
        "id": "v3vaVhSkjQ03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['article'][0]"
      ],
      "metadata": {
        "id": "Nhpmp3i_D-3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis(EDA)"
      ],
      "metadata": {
        "id": "HGrg1DBIIzlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "0s7DYfTroECt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "id": "O_URmwNSoVsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "KA_AhE_Zoltl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "id": "aQielPQQpOrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#abstract word count per row\n",
        "df_train['abstract_word_count'] = df_train['abstract'].apply(lambda x: len(x.strip().split()))\n",
        "# number of unique words in body\n",
        "df_train['article_unique_words']=df_train['article'].apply(lambda x:len(set(str(x).split())))\n",
        "# word count in body\n",
        "df_train['article_word_count'] = df_train['article'].apply(lambda x: len(str(x).strip().split()))\n",
        "\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "keMufUBW9KAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['article_word_count'].max()"
      ],
      "metadata": {
        "id": "Ireue2w7hOBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if NULL values are present in the dataset along with count of the NULL values\n",
        "for col in df.columns:\n",
        "    print(col, df[col].isnull().sum())"
      ],
      "metadata": {
        "id": "zDw8WasXLqt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "9ZllfVU0L5D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Tdd0r6yqMZuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns = 'section_names',  inplace=True)"
      ],
      "metadata": {
        "id": "O4w-cScyFXX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation of Dataset and Dataloader\n",
        "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n",
        "train_size = 0.8\n",
        "df_train=df.sample(frac=train_size,random_state = 42)\n",
        "df_test=df.drop(df_train.index).reset_index(drop=True)\n",
        "df_train =df_train.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "FZpqRHP5gJyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "id": "w7ulw4ONkOKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device  = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "GUtOhGdKA2jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = 'Dagar/t5-small-science-papers'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)"
      ],
      "metadata": {
        "id": "oyqw-SW1_siD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bart = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
      ],
      "metadata": {
        "id": "IoZjQx75C0X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "\n",
        "  \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
        "  Yield successive batch-sized chunks from list_of_elements.\"\"\"\n",
        "  for i in range(0, len(list_of_elements), batch_size):\n",
        "\n",
        "    yield list_of_elements[i : i + batch_size]\n"
      ],
      "metadata": {
        "id": "oxKELMXkGby2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                               batch_size=4, device=device,\n",
        "                               column_text=\"article\",\n",
        "                               column_summary=\"abstract\"):\n",
        "  article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "  target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "  for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n",
        "\n",
        "\n",
        "    inputs = tokenizer(article_batch, max_length=1024,  truncation=True,padding=\"max_length\", return_tensors=\"pt\")\n",
        "    summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "                         length_penalty=0.8, num_beams=8, max_length=256)\n",
        "\n",
        "    ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the  token, and add the decoded texts with the references to the metric.\n",
        "    decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "               for s in summaries]\n",
        "\n",
        "    decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "\n",
        "    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "\n",
        "  #  Finally compute and return the ROUGE scores.\n",
        "  score = metric.compute()\n",
        "  return score\n"
      ],
      "metadata": {
        "id": "hZm0Fk_FLSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_token_len = len([tokenizer.encode(s) for s in df_train['article']])\n",
        "\n",
        "summary_token_len = len([tokenizer.encode(s) for s in df_test['abstract']])\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].hist(article_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[0].set_title(\"Article Token Length\")\n",
        "axes[0].set_xlabel(\"Length\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "axes[1].hist(summary_token_len, bins = 20, color = 'C0', edgecolor = 'C0' )\n",
        "axes[1].set_title(\"Summary Token Length\")\n",
        "axes[1].set_xlabel(\"Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BdmddZ5DgZEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    input_encodings = tokenizer(example_batch['article'] , max_length = 1024, truncation = True )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
        "\n",
        "    return {\n",
        "        'input_ids' : input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }\n",
        "dataset_pubmed_pt = df.applymap(convert_examples_to_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "m_OYYPVJicmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_bart)"
      ],
      "metadata": {
        "id": "ZUH0n3Xvimxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Dataset/NLP"
      ],
      "metadata": {
        "id": "4zewpWcjizrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U accelerate\n",
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "6IHlT35wjuH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wLVoNz0Uk0Nj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "tsrJOJFnorVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='bart-pubmed', num_train_epochs=5, warmup_steps=500,\n",
        "    per_device_train_batch_size=4, per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01, logging_steps=10,\n",
        "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ],
      "metadata": {
        "id": "O5t-OE3Tirdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model=model_bart, args=trainer_args,\n",
        "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
        "                  train_dataset=df_train,\n",
        "                  eval_dataset=df_test)"
      ],
      "metadata": {
        "id": "o-sARnCfSYcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "MMJRtQjZTl3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "Om-WnwLWyod5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class scientificPaperSummary(Dataset):\n",
        "\n",
        "  def __init__(self, DataFrame, tokenizer , text_max_token_len, summary_max_token_len):\n",
        "\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data = DataFrame\n",
        "    self.article = self.data.article\n",
        "    self.abstract = self.data.abstract\n",
        "    self.text_max_token_len = text_max_token_len\n",
        "    self.summary_max_token_len = summary_max_token_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.article)\n",
        "\n",
        "  def __getitem__ (self, index):\n",
        "    abstract = str(self.abstract[index])\n",
        "    abstract = ' '.join(abstract.split())\n",
        "\n",
        "    article = str(self.article[index])\n",
        "    article = ' '.join(article.split())\n",
        "\n",
        "\n",
        "    summary_encoding = self.tokenizer.batch_encode_plus([abstract], max_length= self.text_max_token_len, truncation = True,return_tensors='pt')\n",
        "    text_encoding = self.tokenizer.batch_encode_plus([article], max_length= self.summary_max_token_len,truncation = True,return_tensors='pt')\n",
        "\n",
        "    summary_ids = summary_encoding[\"input_ids\"].squeeze()\n",
        "    summary_mask = summary_encoding[\"attention_mask\"].squeeze()\n",
        "    article_ids = text_encoding[\"input_ids\"].squeeze()\n",
        "    article_mask = text_encoding[\"attention_mask\"].squeeze()\n",
        "\n",
        "\n",
        "\n",
        "    return {\n",
        "        'summary_ids' : summary_ids.to(dtype = torch.long),\n",
        "        'summary_mask' : summary_mask.to(dtype = torch.long),\n",
        "        'article_ids' : article_ids.to(dtype = torch.long),\n",
        "        'article_mask' : article_mask.to(dtype = torch.long)\n",
        "\n",
        "    }\n"
      ],
      "metadata": {
        "id": "sZBR4fngE7Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "\n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "metadata": {
        "id": "YQbDsTP9xInW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "  model.eval()\n",
        "  predictions = []\n",
        "  actuals = []\n",
        "  with torch.no_grad():\n",
        "    for _, data in enumerate(loader, 0):\n",
        "      y = data['summary_ids'].to(device, dtype = torch.long)\n",
        "      ids = data['article_ids'].to(device, dtype = torch.long)\n",
        "      mask = data['article_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "      generated_ids = model.generate(\n",
        "          input_ids = ids,\n",
        "          attention_mask = mask,\n",
        "          max_length=150,\n",
        "          num_beams=2,\n",
        "          repetition_penalty=2.5,\n",
        "          length_penalty=1.0,\n",
        "          early_stopping=True)\n",
        "      preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "      target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "      if _%100==0:\n",
        "        print(f'Completed {_}')\n",
        "      predictions.extend(preds)\n",
        "      actuals.extend(target)\n",
        "  return predictions, actuals"
      ],
      "metadata": {
        "id": "NwjQ9tr12rYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"t5-small\"\n"
      ],
      "metadata": {
        "id": "6EvIzlcei9Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n"
      ],
      "metadata": {
        "id": "EIHRTzM62jXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "0FocYLkz4b1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtNs9ytpCow2"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "  wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "  # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "  # Defining some key variables that will be used later on in the training\n",
        "  config = wandb.config          # Initialize config\n",
        "  config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
        "  config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
        "  config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
        "  config.VAL_EPOCHS = 1\n",
        "  config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "  config.SEED = 42               # random seed (default: 42)\n",
        "  config.MAX_LEN = 512\n",
        "  config.SUMMARY_LEN = 150\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "  torch.manual_seed(config.SEED) # pytorch random seed\n",
        "  np.random.seed(config.SEED) # numpy random seed\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "  # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "  training_set = scientificPaperSummary(df_train, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "  val_set = scientificPaperSummary(df_test, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  # Defining the parameters for creation of dataloaders\n",
        "  train_params = {\n",
        "      'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0}\n",
        "\n",
        "  val_params = {\n",
        "      'batch_size': config.VALID_BATCH_SIZE,\n",
        "      'shuffle': False,\n",
        "      'num_workers': 0}\n",
        "\n",
        "\n",
        "\n",
        "  # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "  training_loader = DataLoader(df_train, **train_params)\n",
        "  val_loader = DataLoader(df_test, **val_params)\n",
        "\n",
        "  model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "\n",
        "  model = model.to(device)\n",
        "      # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "  optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # Log metrics with wandb\n",
        "  wandb.watch(model, log=\"all\")\n",
        "    # Training loop\n",
        "  print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "  for epoch in range(config.TRAIN_EPOCHS):\n",
        "    train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "  print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "  for epoch in range(config.VAL_EPOCHS):\n",
        "    predictions, actuals = validate(epoch, tokenizer, device, model, val_loader)\n",
        "    final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "    final_df.to_csv('./models/predictions.csv')\n",
        "    print('Output Files generated for review')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits.shape, lm_labels.shape)"
      ],
      "metadata": {
        "id": "hPZQNKxwmNq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sciSummaryDataModule(pl.LightningDataModule):\n",
        "  def __init__ (\n",
        "      self,\n",
        "      df_train = pd.DataFrame,\n",
        "      df_test = pd.DataFrame,\n",
        "      tokenizer = T5Tokenizer,\n",
        "      batch_size = 32,\n",
        "      text_max_token_len = 512,\n",
        "      summary_max_token_len = 128\n",
        "  ):\n",
        "\n",
        "\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      self.df_train = df_train\n",
        "      self.df_test = df_test\n",
        "\n",
        "      self.batch_size = batch_size\n",
        "      self.tokenizer = tokenizer\n",
        "      self.text_max_token_len = text_max_token_len\n",
        "      self.summary_max_token_len = summary_max_token_len\n",
        "\n",
        "  def setup(self, stage = None):\n",
        "    self.train_dataset = scientificPaperSummary(\n",
        "        self.df_train,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "\n",
        "    self.test_dataset = scientificPaperSummary(\n",
        "        self.df_test,\n",
        "        self.tokenizer,\n",
        "        self.text_max_token_len,\n",
        "        self.summary_max_token_len\n",
        "    )\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return Dataset(\n",
        "        self.train_dataset,\n",
        "        batch_size = self.batch_size,\n",
        "        shuffle = True,\n",
        "        num_worker =2\n",
        "    )\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return Dataset(\n",
        "        self.test_dataset,\n",
        "        batch_size = self.batch_size,\n",
        "        shuffle = False,\n",
        "        num_worker =2\n",
        "    )"
      ],
      "metadata": {
        "id": "5sVEhHWHhs-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_token_counts, summary_token_counts= [], []\n",
        "\n",
        "for _, row in df_train.iterrows():\n",
        "  text_token_count = len(tokenizer.encode(row[\"article\"]))\n",
        "  text_token_counts.append(text_token_count)\n",
        "\n",
        "  summary_token_count = len(tokenizer.encode(row[\"abstract\"]))\n",
        "  summary_token_counts.append(summary_token_count)"
      ],
      "metadata": {
        "id": "Z7AuXORWWu_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_token_counts"
      ],
      "metadata": {
        "id": "yKiIUqbiQgQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot summary_token_counts in one figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(summary_token_counts, marker='o', color='b', label='Summary Token Counts')\n",
        "plt.xlabel('Number of papers')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Summary Token Counts')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot text_token_counts in another figure\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(text_token_counts, marker='o', color='g', label='Text Token Counts')\n",
        "plt.xlabel('Number of papers')\n",
        "plt.ylabel('Counts')\n",
        "plt.title('Text Token Counts')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s6yx3ZN4sSsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SciPaperSummaryModel(pl.LightningModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_attention_mask, labels=None):\n",
        "        output = self.model(input_ids, attention_mask=attention_mask, labels=labels, decoder_attention_mask=decoder_attention_mask)\n",
        "        return output.loss, output.logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"text_input_ids\"]\n",
        "        attention_mask = batch[\"text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, decoder_attention_mask=labels_attention_mask, labels=labels)\n",
        "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"text_input_ids\"]\n",
        "        attention_mask = batch[\"text_attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        labels_attention_mask = batch[\"labels_attention_mask\"]\n",
        "\n",
        "        loss, outputs = self(input_ids=input_ids, attention_mask=attention_mask, decoder_attention_mask=labels_attention_mask, labels=labels)\n",
        "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return AdamW(self.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "pVHD42lXVDqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "ECdzTwuOMJd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch = 10\n",
        "batch_size = 32\n",
        "\n",
        "data_module = sciSummaryDataModule(df_train, df_test, tokenizer, batch_size = batch_size)"
      ],
      "metadata": {
        "id": "EYFQt_p8WxX4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}